{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778aae66",
   "metadata": {},
   "source": [
    "# Обробка даних у Інтелектуальному Аналізі Даних\n",
    "\n",
    "## Типи даних\n",
    "\n",
    "Типи даних є фундаментальними будівельними блоками будь-якого проєкту, який працює з даними. Розуміння природи ваших даних є критично важливим для вибору відповідних технік попередньої обробки, алгоритмів та аналітичних методів. У цій лекції ми розглянемо різні типи даних, з якими ви зіткнетеся в data mining, та як ефективно з ними працювати.\n",
    "\n",
    "![Data Types](images/3.0-data-types.png)\n",
    "\n",
    "[Джерело зображення](https://www.mygreatlearning.com/blog/types-of-data/)\n",
    "\n",
    "### 1. Числові типи даних\n",
    "\n",
    "Числові дані представляють кількості та можуть бути виміряні на числовій шкалі. Існує два основних підтипи:\n",
    "\n",
    "#### Неперервні числові дані\n",
    "- **Визначення**: Дані, які можуть приймати будь-яке значення в межах діапазону (включно з десятковими значеннями)\n",
    "- **Приклади**: Зріст (1.75м), температура (23.4°C), дохід ($45,678.90), час (2.5 години)\n",
    "- **Характеристики**: \n",
    "  - Нескінченна кількість можливих значень в межах діапазону\n",
    "  - Можуть бути виміряні з високою точністю\n",
    "  - Підтримують арифметичні операції (додавання, віднімання, множення, ділення)\n",
    "  - Можуть бути агреговані за допомогою середнього, медіани, стандартного відхилення\n",
    "\n",
    "#### Дискретні числові дані\n",
    "- **Визначення**: Дані, які можуть приймати лише конкретні, підрахункові значення (зазвичай цілі числа)\n",
    "- **Приклади**: Кількість дітей (0, 1, 2, 3...), кількість проданих товарів, кількість відвідувань\n",
    "- **Характеристики**:\n",
    "  - Скінченна або зліченно нескінченна кількість можливих значень\n",
    "  - Часто представляють підрахунки або рейтинги\n",
    "  - Підтримують арифметичні операції, але з обмеженнями\n",
    "  - Можуть бути агреговані за допомогою середнього, медіани, моди\n",
    "\n",
    "### 2. Категоріальні типи даних\n",
    "\n",
    "Категоріальні дані представляють групи або категорії та не можуть бути виміряні на числовій шкалі. Існує два основних підтипи:\n",
    "\n",
    "#### Номінальні категоріальні дані\n",
    "- **Визначення**: Категорії без будь-якого впорядкування або ранжування\n",
    "- **Приклади**: Стать (Чоловік, Жінка, Інше), Колір (Червоний, Синій, Зелений), Країна (США, Канада, Мексика)\n",
    "- **Характеристики**:\n",
    "  - Немає значущого порядку між категоріями\n",
    "  - Категорії взаємно виключні\n",
    "  - Неможливо виконувати арифметичні операції\n",
    "  - Можна лише підраховувати частоти та обчислювати пропорції\n",
    "\n",
    "#### Ординальні категоріальні дані\n",
    "- **Визначення**: Категорії зі значущим порядком або ранжуванням\n",
    "- **Приклади**: Рівень освіти (Середня школа → Бакалавр → Магістр → Доктор), Оцінка (Погано, Задовільно, Добре, Відмінно), Розмір (Малий, Середній, Великий)\n",
    "- **Характеристики**:\n",
    "  - Категорії мають чіткий порядок\n",
    "  - Відстань між категоріями може бути нерівною\n",
    "  - Можуть бути ранжовані, але арифметичні операції обмежені\n",
    "  - Можуть бути перетворені на числові шкали для аналізу\n",
    "\n",
    "### 3. Спеціальні типи даних\n",
    "\n",
    "#### Бінарні дані\n",
    "- **Визначення**: Дані з точно двома можливими значеннями\n",
    "- **Приклади**: Так/Ні, Істина/Хибність, 0/1, Здано/Не здано\n",
    "- **Характеристики**:\n",
    "  - Найпростіша форма категоріальних даних\n",
    "  - Можуть розглядатися як числові (0/1) для аналізу\n",
    "  - Часто використовуються як цільові змінні в класифікації\n",
    "\n",
    "#### Текстові дані\n",
    "- **Визначення**: Неструктурована текстова інформація\n",
    "- **Приклади**: Описи продуктів, відгуки клієнтів, пости в соціальних мережах\n",
    "- **Характеристики**:\n",
    "  - Вимагають спеціальної попередньої обробки (токенізація, стемінг тощо)\n",
    "  - Можуть бути перетворені на числові ознаки за допомогою різних технік\n",
    "  - Часто аналізуються за допомогою методів обробки природної мови\n",
    "\n",
    "#### Дані дати/часу\n",
    "- **Визначення**: Часова інформація\n",
    "- **Приклади**: Дати народження, часові мітки, час призначених зустрічей\n",
    "- **Характеристики**:\n",
    "  - Можуть бути розкладені на кілька ознак (рік, місяць, день, година)\n",
    "  - Підтримують часовий аналіз та методи часових рядів\n",
    "  - Часто вимагають спеціальної обробки для часових поясів та форматів\n",
    "\n",
    "### 4. Типові проблеми та застереження щодо типів даних\n",
    "\n",
    "#### Неправильна ідентифікація типу\n",
    "- **Проблема**: Обробка категоріальних даних як числових\n",
    "- **Приклад**: Поштові індекси (12345) виглядають як числові, але є категоріальними\n",
    "- **Рішення**: Перевірити, чи мають сенс арифметичні операції\n",
    "\n",
    "#### Змішані типи даних\n",
    "- **Проблема**: Один стовпець містить кілька типів даних\n",
    "- **Приклад**: \"25\", \"Невідомо\", \"N/A\" в стовпці віку\n",
    "- **Рішення**: Очищення та стандартизація даних\n",
    "\n",
    "#### Проблеми кодування\n",
    "- **Проблема**: Категоріальні дані зберігаються як числа\n",
    "- **Приклад**: Стать зберігається як 1/2 замість Чоловік/Жінка\n",
    "- **Рішення**: Правильне маркування та документація\n",
    "\n",
    "#### Втрата точності\n",
    "- **Проблема**: Округлення неперервних даних до дискретних\n",
    "- **Приклад**: Зберігання зросту 1.75м як 2м\n",
    "- **Рішення**: Підтримувати відповідні рівні точності\n",
    "\n",
    "#### Представлення відсутніх значень\n",
    "- **Проблема**: Неконсистентне представлення відсутніх даних\n",
    "- **Приклад**: \"N/A\", \"NULL\", \"\", \"0\" всі представляють відсутні значення\n",
    "- **Рішення**: Стандартизувати представлення відсутніх значень\n",
    "\n",
    "### 5. Коли використовувати перетворення типів даних\n",
    "\n",
    "**Вимоги алгоритмів**\n",
    "- Більшість алгоритмів машинного навчання вимагають числового вводу\n",
    "- Статистичний аналіз часто потребує числових даних\n",
    "- Алгоритми на основі відстані (k-means, k-NN) потребують числових ознак\n",
    "\n",
    "**Ординальні дані зі значущим порядком**\n",
    "- Рівні освіти (Середня школа → Бакалавр → Магістр → Доктор)\n",
    "- Оцінки задоволеності клієнтів (Погано → Задовільно → Добре → Відмінно)\n",
    "- Дохідні категорії (Низький → Середній → Високий)\n",
    "\n",
    "**Категоріальні дані з високою кардинальністю**\n",
    "- Коли у вас багато категорій і one-hot кодування створить занадто багато ознак\n",
    "- Використовуйте target encoding або техніки вбудовування\n",
    "\n",
    "**Оптимізація продуктивності**\n",
    "- Зменшити використання пам'яті шляхом перетворення текстових категорій на числові коди\n",
    "- Прискорити обчислення у великих наборах даних\n",
    "\n",
    "#### Коли перетворювати числові на категоріальні\n",
    "\n",
    "**Нелинійні зв'язки**\n",
    "- Коли зв'язок між числовою змінною та ціллю не є лінійним\n",
    "- Вікові групи (0-18, 19-35, 36-50, 50+) замість неперервного віку\n",
    "\n",
    "**Вимоги бізнес-логіки**\n",
    "- Створення значущих бізнес-сегментів\n",
    "- Категорії ризику на основі діапазонів балів\n",
    "- Відповідність регуляторним вимогам\n",
    "\n",
    "**Запобігання перенавчанню**\n",
    "- Групування подібних значень для зменшення шуму\n",
    "- Створення більш надійних моделей з меншою кількістю параметрів\n",
    "\n",
    "**Інтерпретованість**\n",
    "- Зробити результати більш зрозумілими для бізнес-стейкхолдерів\n",
    "- Створення дієвих інсайтів з неперервних змінних"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241696ee",
   "metadata": {},
   "source": [
    "## Очищення даних\n",
    "\n",
    "Очищення даних є одним з найкритичніших кроків у процесі data mining. Реальні дані часто бувають брудними, неповними та неконсистентними.\n",
    "\n",
    "### 1. Обробка відсутніх значень\n",
    "\n",
    "Відсутні значення є однією з найпоширеніших проблем якості даних. Вони можуть виникати через помилки збору даних, збої системи або просто тому, що інформація не була доступна.\n",
    "\n",
    "![Missing values](images/3.1-missing-values.png)\n",
    "\n",
    "[Джерело зображення](https://blog.dailydoseofds.com/p/3-types-of-missing-values)\n",
    "\n",
    "#### Типи відсутніх даних\n",
    "\n",
    "**Відсутні повністю випадково (MCAR)**\n",
    "- Відсутні значення не пов'язані з жодними спостережуваними або неспостережуваними змінними\n",
    "- Приклад: Випадковий збій системи, що спричиняє втрату даних\n",
    "\n",
    "**Відсутні випадково (MAR)**\n",
    "- Відсутні значення пов'язані зі спостережуваними змінними, але не з самими відсутніми значеннями\n",
    "- Приклад: Дані про високий дохід відсутні для молодших людей (вік спостережується)\n",
    "\n",
    "**Відсутні не випадково (MNAR)**\n",
    "- Відсутні значення навмисні\n",
    "- Приклад: Люди з дуже високим доходом відмовляються його вказувати\n",
    "\n",
    "#### Обробка відсутніх значень\n",
    "\n",
    "**1. Методи видалення**\n",
    "- **Listwise deletion**: Видалення всіх рядків з будь-якими відсутніми значеннями\n",
    "- **Pairwise deletion**: Використання доступних даних для кожного аналізу\n",
    "- **Column deletion**: Видалення стовпців з занадто великою кількістю відсутніх значень\n",
    "\n",
    "**2. Методи імпутації**\n",
    "- **Mean/Median imputation**: Заміна центральною тенденцією\n",
    "- **Mode imputation**: Заміна найчастішим значенням\n",
    "- **Forward/Backward fill**: Використання попередніх/наступних значень (для часових рядів)\n",
    "- **Constant imputation**: Заміна фіксованим значенням\n",
    "\n",
    "**3. Розширені методи**\n",
    "- **Інтерполяція**: Апроксимація значень між відомими точками\n",
    "- **Регресійна імпутація**: Використання інших змінних для прогнозування відсутніх значень"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49b70649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset with missing values:\n",
      "    age   income education     city\n",
      "0  25.0  50000.0  Bachelor      NYC\n",
      "1  30.0  60000.0    Master       LA\n",
      "2   NaN  70000.0       PhD  Chicago\n",
      "3  35.0      NaN  Bachelor      NYC\n",
      "4  28.0  55000.0       NaN       LA\n",
      "5   NaN  80000.0    Master      NaN\n",
      "6  42.0      NaN       PhD  Chicago\n",
      "\n",
      "Missing values per column:\n",
      "age          2\n",
      "income       2\n",
      "education    1\n",
      "city         1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing Values - Practical Examples\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Create a toy dataset with missing values\n",
    "data = {\n",
    "    'age': [25, 30, np.nan, 35, 28, np.nan, 42],\n",
    "    'income': [50000, 60000, 70000, np.nan, 55000, 80000, np.nan],\n",
    "    'education': ['Bachelor', 'Master', 'PhD', 'Bachelor', np.nan, 'Master', 'PhD'],\n",
    "    'city': ['NYC', 'LA', 'Chicago', 'NYC', 'LA', np.nan, 'Chicago']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original dataset with missing values:\")\n",
    "print(df)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4545696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DELETION METHODS ===\n",
      "Listwise deletion (remove rows with ANY missing values):\n",
      "    age   income education city\n",
      "0  25.0  50000.0  Bachelor  NYC\n",
      "1  30.0  60000.0    Master   LA\n",
      "\n",
      "Column deletion (remove columns with >50.0% missing):\n",
      "    age   income education     city\n",
      "0  25.0  50000.0  Bachelor      NYC\n",
      "1  30.0  60000.0    Master       LA\n",
      "2   NaN  70000.0       PhD  Chicago\n",
      "3  35.0      NaN  Bachelor      NYC\n",
      "4  28.0  55000.0       NaN       LA\n",
      "5   NaN  80000.0    Master      NaN\n",
      "6  42.0      NaN       PhD  Chicago\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Deletion\n",
    "print(\"=== DELETION METHODS ===\")\n",
    "\n",
    "# Listwise deletion (remove rows with any missing values)\n",
    "df_listwise = df.dropna()\n",
    "print(\"Listwise deletion (remove rows with ANY missing values):\")\n",
    "print(df_listwise)\n",
    "\n",
    "# Column deletion (remove columns with too many missing values)\n",
    "threshold = 0.5  # Remove columns with >50% missing values\n",
    "df_column_clean = df.dropna(axis=1, thresh=len(df) * (1 - threshold))\n",
    "print(f\"\\nColumn deletion (remove columns with >{threshold*100}% missing):\")\n",
    "print(df_column_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2502a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== IMPUTATION METHODS ===\n",
      "Mean imputation:\n",
      "    age   income education     city\n",
      "0  25.0  50000.0  Bachelor      NYC\n",
      "1  30.0  60000.0    Master       LA\n",
      "2  32.0  70000.0       PhD  Chicago\n",
      "3  35.0  63000.0  Bachelor      NYC\n",
      "4  28.0  55000.0       NaN       LA\n",
      "5  32.0  80000.0    Master      NaN\n",
      "6  42.0  63000.0       PhD  Chicago\n",
      "\n",
      "Median imputation:\n",
      "    age   income education     city\n",
      "0  25.0  50000.0  Bachelor      NYC\n",
      "1  30.0  60000.0    Master       LA\n",
      "2  30.0  70000.0       PhD  Chicago\n",
      "3  35.0  60000.0  Bachelor      NYC\n",
      "4  28.0  55000.0       NaN       LA\n",
      "5  30.0  80000.0    Master      NaN\n",
      "6  42.0  60000.0       PhD  Chicago\n",
      "\n",
      "Mode imputation for categorical:\n",
      "    age   income education     city\n",
      "0  25.0  50000.0  Bachelor      NYC\n",
      "1  30.0  60000.0    Master       LA\n",
      "2   NaN  70000.0       PhD  Chicago\n",
      "3  35.0      NaN  Bachelor      NYC\n",
      "4  28.0  55000.0  Bachelor       LA\n",
      "5   NaN  80000.0    Master  Chicago\n",
      "6  42.0      NaN       PhD  Chicago\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Imputation\n",
    "print(\"=== IMPUTATION METHODS ===\")\n",
    "\n",
    "# Mean imputation for numeric columns\n",
    "df_mean = df.copy()\n",
    "df_mean['age'].fillna(df_mean['age'].mean(), inplace=True)\n",
    "df_mean['income'].fillna(df_mean['income'].mean(), inplace=True)\n",
    "print(\"Mean imputation:\")\n",
    "print(df_mean)\n",
    "\n",
    "# Median imputation (more robust to outliers)\n",
    "df_median = df.copy()\n",
    "df_median['age'].fillna(df_median['age'].median(), inplace=True)\n",
    "df_median['income'].fillna(df_median['income'].median(), inplace=True)\n",
    "print(\"\\nMedian imputation:\")\n",
    "print(df_median)\n",
    "\n",
    "# Mode imputation for categorical columns\n",
    "df_mode = df.copy()\n",
    "df_mode['education'].fillna(df_mode['education'].mode()[0], inplace=True)\n",
    "df_mode['city'].fillna(df_mode['city'].mode()[0], inplace=True)\n",
    "print(\"\\nMode imputation for categorical:\")\n",
    "print(df_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5acd76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FORWARD/BACKWARD FILL ===\n",
      "Original time series:\n",
      "        date  sales\n",
      "0 2023-01-01  100.0\n",
      "1 2023-01-02  120.0\n",
      "2 2023-01-03    NaN\n",
      "3 2023-01-04  140.0\n",
      "4 2023-01-05    NaN\n",
      "5 2023-01-06  160.0\n",
      "6 2023-01-07  180.0\n",
      "\n",
      "Forward fill:\n",
      "        date  sales\n",
      "0 2023-01-01  100.0\n",
      "1 2023-01-02  120.0\n",
      "2 2023-01-03  120.0\n",
      "3 2023-01-04  140.0\n",
      "4 2023-01-05  140.0\n",
      "5 2023-01-06  160.0\n",
      "6 2023-01-07  180.0\n",
      "\n",
      "Backward fill:\n",
      "        date  sales\n",
      "0 2023-01-01  100.0\n",
      "1 2023-01-02  120.0\n",
      "2 2023-01-03  140.0\n",
      "3 2023-01-04  140.0\n",
      "4 2023-01-05  160.0\n",
      "5 2023-01-06  160.0\n",
      "6 2023-01-07  180.0\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Forward/Backward Fill (for time series data)\n",
    "print(\"=== FORWARD/BACKWARD FILL ===\")\n",
    "\n",
    "# Create time series data with missing values\n",
    "ts_data = pd.DataFrame({\n",
    "    'date': pd.date_range('2023-01-01', periods=7),\n",
    "    'sales': [100, 120, np.nan, 140, np.nan, 160, 180]\n",
    "})\n",
    "\n",
    "print(\"Original time series:\")\n",
    "print(ts_data)\n",
    "\n",
    "# Forward fill\n",
    "ts_forward = ts_data.copy()\n",
    "ts_forward['sales'].fillna(method='ffill', inplace=True)\n",
    "print(\"\\nForward fill:\")\n",
    "print(ts_forward)\n",
    "\n",
    "# Backward fill\n",
    "ts_backward = ts_data.copy()\n",
    "ts_backward['sales'].fillna(method='bfill', inplace=True)\n",
    "print(\"\\nBackward fill:\")\n",
    "print(ts_backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466812d1",
   "metadata": {},
   "source": [
    "### 2. Обробка викидів\n",
    "\n",
    "Викиди - це точки даних, які значно відрізняються від інших спостережень. Вони можуть бути справжніми точками даних або помилками, які потребують вирішення.\n",
    "\n",
    "#### Типи викидів\n",
    "\n",
    "**Точкові викиди**\n",
    "- Окремі точки даних, які є незвичайними\n",
    "- Приклад: Людина з віком 200 років\n",
    "\n",
    "**Контекстуальні викиди**\n",
    "- Точки даних, які є незвичайними в конкретному контексті\n",
    "- Приклад: Носіння зимового пальта влітку\n",
    "\n",
    "**Колективні викиди**\n",
    "- Сукупність точок даних, які є незвичайними разом\n",
    "- Приклад: Кілька послідовних нульових значень у показаннях датчика\n",
    "\n",
    "#### Методи виявлення викидів\n",
    "\n",
    "**1. Статистичні методи**\n",
    "- **Z-score**: Значення за межами ±3 стандартних відхилень\n",
    "- **Метод IQR**: Значення поза Q1 - 1.5×IQR або Q3 + 1.5×IQR\n",
    "- **Модифікований Z-score**: Більш стійкий до викидів за допомогою медіани\n",
    "\n",
    "**2. Візуальні методи**\n",
    "- **Box plots**: Візуальна ідентифікація викидів\n",
    "- **Scatter plots**: Виявлення незвичайних патернів\n",
    "- **Гістограми**: Ідентифікація незвичайних розподілів\n",
    "\n",
    "**3. Предметні знання**\n",
    "- Розуміння того, які значення є розумними\n",
    "- Бізнес-правила та обмеження"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3329b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with outliers:\n",
      "Mean: 52.85\n",
      "Std: 33.04\n",
      "Min: -50.0\n",
      "Max: 300.0\n"
     ]
    }
   ],
   "source": [
    "# Outlier Detection and Handling\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create dataset with outliers\n",
    "np.random.seed(42)\n",
    "normal_data = np.random.normal(50, 10, 100)\n",
    "outlier_data = [150, 200, -50, 300]  # Clear outliers\n",
    "data_with_outliers = np.concatenate([normal_data, outlier_data])\n",
    "\n",
    "df_outliers = pd.DataFrame({'values': data_with_outliers})\n",
    "print(\"Dataset with outliers:\")\n",
    "print(f\"Mean: {df_outliers['values'].mean():.2f}\")\n",
    "print(f\"Std: {df_outliers['values'].std():.2f}\")\n",
    "print(f\"Min: {df_outliers['values'].min()}\")\n",
    "print(f\"Max: {df_outliers['values'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35304107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Z-SCORE METHOD ===\n",
      "Outliers detected by Z-score (>3): 3\n",
      "[200. -50. 300.]\n",
      "\n",
      "=== IQR METHOD ===\n",
      "Outliers detected by IQR: 5\n",
      "Bounds: [27.17, 72.03]\n",
      "[ 23.80254896 150.         200.         -50.         300.        ]\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Z-score method\n",
    "print(\"=== Z-SCORE METHOD ===\")\n",
    "z_scores = np.abs((df_outliers['values'] - df_outliers['values'].mean()) / df_outliers['values'].std())\n",
    "outliers_z = df_outliers[z_scores > 3]\n",
    "print(f\"Outliers detected by Z-score (>3): {len(outliers_z)}\")\n",
    "print(outliers_z['values'].values)\n",
    "\n",
    "# Method 2: IQR method\n",
    "print(\"\\n=== IQR METHOD ===\")\n",
    "Q1 = df_outliers['values'].quantile(0.25)\n",
    "Q3 = df_outliers['values'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers_iqr = df_outliers[(df_outliers['values'] < lower_bound) | (df_outliers['values'] > upper_bound)]\n",
    "print(f\"Outliers detected by IQR: {len(outliers_iqr)}\")\n",
    "print(f\"Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "print(outliers_iqr['values'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6c70a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OUTLIER HANDLING METHODS ===\n",
      "After removing outliers: 99 rows\n",
      "New mean: 49.22\n",
      "\n",
      "After capping outliers: 104 rows\n",
      "New mean: 49.45\n",
      "\n",
      "After log transformation: 104 rows\n",
      "New mean: 3.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/roman/.pyenv/versions/3.10.10/envs/data-mining-course/lib/python3.10/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Outlier Handling Methods\n",
    "print(\"=== OUTLIER HANDLING METHODS ===\")\n",
    "\n",
    "# Method 1: Remove outliers\n",
    "df_no_outliers = df_outliers[(df_outliers['values'] >= lower_bound) & (df_outliers['values'] <= upper_bound)]\n",
    "print(f\"After removing outliers: {len(df_no_outliers)} rows\")\n",
    "print(f\"New mean: {df_no_outliers['values'].mean():.2f}\")\n",
    "\n",
    "# Method 2: Cap outliers (winsorization)\n",
    "df_capped = df_outliers.copy()\n",
    "df_capped['values'] = df_capped['values'].clip(lower=lower_bound, upper=upper_bound)\n",
    "print(f\"\\nAfter capping outliers: {len(df_capped)} rows\")\n",
    "print(f\"New mean: {df_capped['values'].mean():.2f}\")\n",
    "\n",
    "# Method 3: Transform outliers (log transformation)\n",
    "df_log = df_outliers.copy()\n",
    "df_log['values'] = np.log1p(df_log['values'])  # log1p handles zeros\n",
    "print(f\"\\nAfter log transformation: {len(df_log)} rows\")\n",
    "print(f\"New mean: {df_log['values'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0dbc1",
   "metadata": {},
   "source": [
    "### 3. Нормалізація та стандартизація,\n",
    "\n",
    "Стандартизація, та нормалізація (масштабування) є критично важливими для алгоритмів, які чутливі до масштабу ознак. Різні методи підходять для різних сценаріїв.\n",
    "\n",
    "![Scaling](images/3.2-stand-norm.png)\n",
    "\n",
    "[Джерело зображення](kdfoundation.org/?k=278075716)\n",
    "\n",
    "#### Коли використовувати кожен метод\n",
    "\n",
    "**Стандартизація (Z-score нормалізація)**\n",
    "- **Коли використовувати**: Коли дані слідують нормальному розподілу\n",
    "- **Формула**: (x - μ) / σ\n",
    "- **Результат**: Середнє = 0, Стандартне відхилення = 1\n",
    "- **Найкраще для**: Більшості алгоритмів машинного навчання, особливо коли ознаки мають різні масштаби\n",
    "\n",
    "**Min-Max масштабування**\n",
    "- **Коли використовувати**: Коли ви знаєте межі даних і хочете зберегти оригінальний розподіл\n",
    "- **Формула**: (x - min) / (max - min)\n",
    "- **Результат**: Значення між 0 і 1\n",
    "- **Найкраще для**: Нейронних мереж, алгоритмів, які вимагають обмеженого вводу\n",
    "\n",
    "**Robust масштабування**\n",
    "- **Коли використовувати**: Коли дані мають викиди\n",
    "- **Формула**: (x - median) / IQR\n",
    "- **Результат**: Більш стійкий до викидів, ніж стандартизація\n",
    "- **Найкраще для**: Даних зі значними викидами\n",
    "\n",
    "**Масштабування одиничного вектора**\n",
    "- **Коли використовувати**: Коли ви хочете нормалізувати довжину векторів ознак\n",
    "- **Формула**: x / ||x||\n",
    "- **Результат**: Кожен зразок має одиничну норму\n",
    "- **Найкраще для**: Класифікації текстів, косинусної схожості"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f2a17d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data with different scales:\n",
      "   age  income  score\n",
      "0   25   30000   0.10\n",
      "1   30   45000   0.30\n",
      "2   35   60000   0.50\n",
      "3   40   75000   0.70\n",
      "4   45   90000   0.80\n",
      "5   50  120000   0.90\n",
      "6   55  150000   0.95\n",
      "7   60  200000   1.00\n",
      "\n",
      "Original statistics:\n",
      "             age         income     score\n",
      "count   8.000000       8.000000  8.000000\n",
      "mean   42.500000   96250.000000  0.656250\n",
      "std    12.247449   57367.860589  0.326713\n",
      "min    25.000000   30000.000000  0.100000\n",
      "25%    33.750000   56250.000000  0.450000\n",
      "50%    42.500000   82500.000000  0.750000\n",
      "75%    51.250000  127500.000000  0.912500\n",
      "max    60.000000  200000.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Scaling and Normalization Examples\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "\n",
    "# Create dataset with different scales\n",
    "data_scaling = {\n",
    "    'age': [25, 30, 35, 40, 45, 50, 55, 60],\n",
    "    'income': [30000, 45000, 60000, 75000, 90000, 120000, 150000, 200000],\n",
    "    'score': [0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95, 1.0]\n",
    "}\n",
    "\n",
    "df_scaling = pd.DataFrame(data_scaling)\n",
    "print(\"Original data with different scales:\")\n",
    "print(df_scaling)\n",
    "print(f\"\\nOriginal statistics:\")\n",
    "print(df_scaling.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c01100f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STANDARDIZATION (Z-SCORE) ===\n",
      "Standardized data (mean=0, std=1):\n",
      "        age    income     score\n",
      "0 -1.527525 -1.234563 -1.820121\n",
      "1 -1.091089 -0.955039 -1.165695\n",
      "2 -0.654654 -0.675516 -0.511270\n",
      "3 -0.218218 -0.395992  0.143156\n",
      "4  0.218218 -0.116468  0.470368\n",
      "5  0.654654  0.442579  0.797581\n",
      "6  1.091089  1.001626  0.961187\n",
      "7  1.527525  1.933372  1.124794\n",
      "\n",
      "Standardized statistics:\n",
      "            age    income         score\n",
      "count  8.000000  8.000000  8.000000e+00\n",
      "mean   0.000000  0.000000 -5.551115e-17\n",
      "std    1.069045  1.069045  1.069045e+00\n",
      "min   -1.527525 -1.234563 -1.820121e+00\n",
      "25%   -0.763763 -0.745396 -6.748763e-01\n",
      "50%    0.000000 -0.256230  3.067619e-01\n",
      "75%    0.763763  0.582341  8.384826e-01\n",
      "max    1.527525  1.933372  1.124794e+00\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Standardization (Z-score)\n",
    "print(\"=== STANDARDIZATION (Z-SCORE) ===\")\n",
    "scaler_std = StandardScaler()\n",
    "df_std = pd.DataFrame(scaler_std.fit_transform(df_scaling), \n",
    "                     columns=df_scaling.columns, \n",
    "                     index=df_scaling.index)\n",
    "print(\"Standardized data (mean=0, std=1):\")\n",
    "print(df_std)\n",
    "print(f\"\\nStandardized statistics:\")\n",
    "print(df_std.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "867b53d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MIN-MAX SCALING ===\n",
      "Min-Max scaled data (0-1 range):\n",
      "        age    income     score\n",
      "0  0.000000  0.000000  0.000000\n",
      "1  0.142857  0.088235  0.222222\n",
      "2  0.285714  0.176471  0.444444\n",
      "3  0.428571  0.264706  0.666667\n",
      "4  0.571429  0.352941  0.777778\n",
      "5  0.714286  0.529412  0.888889\n",
      "6  0.857143  0.705882  0.944444\n",
      "7  1.000000  1.000000  1.000000\n",
      "\n",
      "Min-Max statistics:\n",
      "            age    income     score\n",
      "count  8.000000  8.000000  8.000000\n",
      "mean   0.500000  0.389706  0.618056\n",
      "std    0.349927  0.337458  0.363014\n",
      "min    0.000000  0.000000  0.000000\n",
      "25%    0.250000  0.154412  0.388889\n",
      "50%    0.500000  0.308824  0.722222\n",
      "75%    0.750000  0.573529  0.902778\n",
      "max    1.000000  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Min-Max Scaling\n",
    "print(\"=== MIN-MAX SCALING ===\")\n",
    "scaler_mm = MinMaxScaler()\n",
    "df_mm = pd.DataFrame(scaler_mm.fit_transform(df_scaling), \n",
    "                    columns=df_scaling.columns, \n",
    "                    index=df_scaling.index)\n",
    "print(\"Min-Max scaled data (0-1 range):\")\n",
    "print(df_mm)\n",
    "print(f\"\\nMin-Max statistics:\")\n",
    "print(df_mm.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6d6d4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ROBUST SCALING ===\n",
      "Robust scaled data (median=0, IQR=1):\n",
      "        age    income     score\n",
      "0 -0.777778 -0.560000 -1.106383\n",
      "1 -0.555556 -0.400000 -0.765957\n",
      "2 -0.333333 -0.240000 -0.425532\n",
      "3 -0.111111 -0.080000 -0.085106\n",
      "4  0.111111  0.080000  0.085106\n",
      "5  0.333333  0.400000  0.255319\n",
      "6  0.555556  0.720000  0.340426\n",
      "7  0.777778  1.253333  0.425532\n",
      "8  2.555556  4.453333 -1.191489\n",
      "9 -1.222222 -0.773333  0.408511\n",
      "\n",
      "Robust statistics:\n",
      "             age     income      score\n",
      "count  10.000000  10.000000  10.000000\n",
      "mean    0.133333   0.485333  -0.205957\n",
      "std     1.049920   1.522716   0.626351\n",
      "min    -1.222222  -0.773333  -1.191489\n",
      "25%    -0.500000  -0.360000  -0.680851\n",
      "50%     0.000000   0.000000   0.000000\n",
      "75%     0.500000   0.640000   0.319149\n",
      "max     2.555556   4.453333   0.425532\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Robust Scaling (with outliers)\n",
    "print(\"=== ROBUST SCALING ===\")\n",
    "# Add outliers to demonstrate robust scaling\n",
    "data_with_outliers = df_scaling.copy()\n",
    "data_with_outliers.loc[8] = [100, 500000, 0.05]  # Add outlier\n",
    "data_with_outliers.loc[9] = [15, 10000, 0.99]    # Add outlier\n",
    "\n",
    "scaler_robust = RobustScaler()\n",
    "df_robust = pd.DataFrame(scaler_robust.fit_transform(data_with_outliers), \n",
    "                        columns=data_with_outliers.columns, \n",
    "                        index=data_with_outliers.index)\n",
    "print(\"Robust scaled data (median=0, IQR=1):\")\n",
    "print(df_robust)\n",
    "print(f\"\\nRobust statistics:\")\n",
    "print(df_robust.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a18ebc",
   "metadata": {},
   "source": [
    "### 4. Дискретизація та бінінг\n",
    "\n",
    "Дискретизація перетворює неперервні змінні на дискретні категорії. Це корисно для створення категоріальних ознак з числових даних.\n",
    "\n",
    "#### Типи бінінгу\n",
    "\n",
    "**Бінінг рівної ширини**\n",
    "- Ділить дані на інтервали рівної ширини\n",
    "- **Коли використовувати**: Коли ви хочете рівномірні розміри інтервалів\n",
    "- **Приклад**: Вікові групи [0-20, 20-40, 40-60, 60-80]\n",
    "\n",
    "**Бінінг рівної частоти (на основі квантилів)**\n",
    "- Ділить дані на біни з рівною кількістю спостережень\n",
    "- **Коли використовувати**: Коли ви хочете збалансовані категорії\n",
    "- **Приклад**: Квартилі доходу\n",
    "\n",
    "**Спеціальний бінінг**\n",
    "- Використовує предметні знання для створення значущих категорій\n",
    "- **Коли використовувати**: Коли бізнес-логіка визначає природні розриви\n",
    "- **Приклад**: Категорії ІМТ [Недостатня вага, Нормальна, Надмірна вага, Ожиріння]\n",
    "\n",
    "**K-means бінінг**\n",
    "- Використовує кластеризацію для знаходження природних груп\n",
    "- **Коли використовувати**: Коли ви хочете межі бінів, визначені даними"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3eaa63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original continuous data:\n",
      "         age        income\n",
      "0  42.450712  10854.278673\n",
      "1  32.926035  17848.546202\n",
      "2  44.715328  18557.749465\n",
      "3  57.845448  14747.979442\n",
      "4  31.487699  20319.923647\n",
      "\n",
      "Age range: -4.3 - 62.8\n",
      "Income range: 8439 - 85827\n"
     ]
    }
   ],
   "source": [
    "# Discretization and Binning Examples\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# Create continuous data for binning\n",
    "np.random.seed(42)\n",
    "age_data = np.random.normal(35, 15, 100)\n",
    "income_data = np.random.lognormal(10, 0.5, 100)\n",
    "\n",
    "df_binning = pd.DataFrame({\n",
    "    'age': age_data,\n",
    "    'income': income_data\n",
    "})\n",
    "\n",
    "print(\"Original continuous data:\")\n",
    "print(df_binning.head())\n",
    "print(f\"\\nAge range: {df_binning['age'].min():.1f} - {df_binning['age'].max():.1f}\")\n",
    "print(f\"Income range: {df_binning['income'].min():.0f} - {df_binning['income'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d363c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EQUAL WIDTH BINNING ===\n",
      "Equal width binning results:\n",
      "         age       age_ew        income income_ew\n",
      "0  42.450712  Middle-aged  10854.278673       Low\n",
      "1  32.926035  Middle-aged  17848.546202       Low\n",
      "2  44.715328  Middle-aged  18557.749465       Low\n",
      "3  57.845448       Senior  14747.979442       Low\n",
      "4  31.487699  Middle-aged  20319.923647       Low\n",
      "5  31.487946  Middle-aged  26957.731772       Low\n",
      "6  58.688192       Senior  56562.016402      High\n",
      "7  46.511521       Senior  24035.541421       Low\n",
      "8  27.957884        Adult  25053.659793       Low\n",
      "9  43.138401  Middle-aged  21221.647371       Low\n",
      "\n",
      "Age bin counts:\n",
      "age_ew\n",
      "Young           6\n",
      "Adult          32\n",
      "Middle-aged    42\n",
      "Senior         20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Income bin counts:\n",
      "income_ew\n",
      "Low          70\n",
      "Medium       23\n",
      "High          5\n",
      "Very High     2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Equal Width Binning\n",
    "print(\"=== EQUAL WIDTH BINNING ===\")\n",
    "df_binning['age_ew'] = pd.cut(df_binning['age'], bins=4, labels=['Young', 'Adult', 'Middle-aged', 'Senior'])\n",
    "df_binning['income_ew'] = pd.cut(df_binning['income'], bins=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "print(\"Equal width binning results:\")\n",
    "print(df_binning[['age', 'age_ew', 'income', 'income_ew']].head(10))\n",
    "print(f\"\\nAge bin counts:\")\n",
    "print(df_binning['age_ew'].value_counts().sort_index())\n",
    "print(f\"\\nIncome bin counts:\")\n",
    "print(df_binning['income_ew'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5548bf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EQUAL FREQUENCY BINNING ===\n",
      "Equal frequency binning results:\n",
      "         age age_ef        income income_ef\n",
      "0  42.450712     Q4  10854.278673        Q1\n",
      "1  32.926035     Q2  17848.546202        Q2\n",
      "2  44.715328     Q4  18557.749465        Q2\n",
      "3  57.845448     Q4  14747.979442        Q2\n",
      "4  31.487699     Q2  20319.923647        Q2\n",
      "5  31.487946     Q2  26957.731772        Q3\n",
      "6  58.688192     Q4  56562.016402        Q4\n",
      "7  46.511521     Q4  24035.541421        Q3\n",
      "8  27.957884     Q2  25053.659793        Q3\n",
      "9  43.138401     Q4  21221.647371        Q2\n",
      "\n",
      "Age quantile bin counts:\n",
      "age_ef\n",
      "Q1    25\n",
      "Q2    25\n",
      "Q3    25\n",
      "Q4    25\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Income quantile bin counts:\n",
      "income_ef\n",
      "Q1    25\n",
      "Q2    25\n",
      "Q3    25\n",
      "Q4    25\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Equal Frequency Binning (Quantile-based)\n",
    "print(\"=== EQUAL FREQUENCY BINNING ===\")\n",
    "df_binning['age_ef'] = pd.qcut(df_binning['age'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "df_binning['income_ef'] = pd.qcut(df_binning['income'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "print(\"Equal frequency binning results:\")\n",
    "print(df_binning[['age', 'age_ef', 'income', 'income_ef']].head(10))\n",
    "print(f\"\\nAge quantile bin counts:\")\n",
    "print(df_binning['age_ef'].value_counts().sort_index())\n",
    "print(f\"\\nIncome quantile bin counts:\")\n",
    "print(df_binning['income_ef'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "288958e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CUSTOM BINNING ===\n",
      "Custom binning results:\n",
      "         age   age_custom        income income_custom\n",
      "0  42.450712        Adult  10854.278673           Low\n",
      "1  32.926035        Adult  17848.546202           Low\n",
      "2  44.715328        Adult  18557.749465           Low\n",
      "3  57.845448  Middle-aged  14747.979442           Low\n",
      "4  31.487699        Adult  20319.923647           Low\n",
      "5  31.487946        Adult  26957.731772  Lower-Middle\n",
      "6  58.688192  Middle-aged  56562.016402        Middle\n",
      "7  46.511521        Adult  24035.541421           Low\n",
      "8  27.957884  Young Adult  25053.659793  Lower-Middle\n",
      "9  43.138401        Adult  21221.647371           Low\n",
      "\n",
      "Custom age bin counts:\n",
      "age_custom\n",
      "Minor          13\n",
      "Young Adult    24\n",
      "Adult          51\n",
      "Middle-aged    11\n",
      "Senior          0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Custom income bin counts:\n",
      "income_custom\n",
      "Low             59\n",
      "Lower-Middle    35\n",
      "Middle           4\n",
      "Upper-Middle     2\n",
      "High             0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Custom Binning (Domain Knowledge)\n",
    "print(\"=== CUSTOM BINNING ===\")\n",
    "# Age groups based on life stages\n",
    "age_bins = [0, 18, 30, 50, 65, 100]\n",
    "age_labels = ['Minor', 'Young Adult', 'Adult', 'Middle-aged', 'Senior']\n",
    "df_binning['age_custom'] = pd.cut(df_binning['age'], bins=age_bins, labels=age_labels)\n",
    "\n",
    "# Income groups based on economic categories\n",
    "income_bins = [0, 25000, 50000, 75000, 100000, float('inf')]\n",
    "income_labels = ['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']\n",
    "df_binning['income_custom'] = pd.cut(df_binning['income'], bins=income_bins, labels=income_labels)\n",
    "\n",
    "print(\"Custom binning results:\")\n",
    "print(df_binning[['age', 'age_custom', 'income', 'income_custom']].head(10))\n",
    "print(f\"\\nCustom age bin counts:\")\n",
    "print(df_binning['age_custom'].value_counts().sort_index())\n",
    "print(f\"\\nCustom income bin counts:\")\n",
    "print(df_binning['income_custom'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe54f2",
   "metadata": {},
   "source": [
    "### 5. Кодування категоріальних змінних\n",
    "\n",
    "Категоріальні змінні потрібно перетворити на числовий формат для більшості алгоритмів машинного навчання. Різні методи кодування підходять для різних типів категоріальних даних.\n",
    "\n",
    "#### Типи кодування\n",
    "\n",
    "**One-Hot Encoding**\n",
    "- Створює бінарні стовпці для кожної категорії\n",
    "- **Коли використовувати**: Номінальні категоріальні дані з невеликою кількістю категорій\n",
    "- **Плюси**: Немає припущення про порядок, зберігає всю інформацію\n",
    "- **Мінуси**: Створює багато стовпців, може спричинити прокляття розмірності\n",
    "\n",
    "**Label Encoding**\n",
    "- Присвоює цілочисельні мітки категоріям\n",
    "- **Коли використовувати**: Ординальні категоріальні дані\n",
    "- **Плюси**: Простий, зберігає порядок\n",
    "- **Мінуси**: Припускає ординальний зв'язок, може ввести алгоритми в оману\n",
    "\n",
    "**Target Encoding**\n",
    "- Використовує статистику цільової змінної для кодування категорій\n",
    "- **Коли використовувати**: Категоріальні дані з високою кардинальністю\n",
    "- **Плюси**: Захоплює зв'язок з ціллю, зменшує розмірність\n",
    "- **Мінуси**: Може спричинити перенавчання, вимагає обережної валідації"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aaa1b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original categorical data:\n",
      "    color    size   brand     rating\n",
      "0     Red   Small    Nike       Poor\n",
      "1    Blue  Medium  Adidas       Fair\n",
      "2   Green   Large    Puma       Good\n",
      "3     Red   Small    Nike  Excellent\n",
      "4    Blue  Medium  Adidas       Good\n",
      "5  Yellow   Large    Nike       Fair\n",
      "6   Green   Small    Puma  Excellent\n"
     ]
    }
   ],
   "source": [
    "# Categorical Encoding Examples\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Create categorical data\n",
    "categorical_data = {\n",
    "    'color': ['Red', 'Blue', 'Green', 'Red', 'Blue', 'Yellow', 'Green'],\n",
    "    'size': ['Small', 'Medium', 'Large', 'Small', 'Medium', 'Large', 'Small'],\n",
    "    'brand': ['Nike', 'Adidas', 'Puma', 'Nike', 'Adidas', 'Nike', 'Puma'],\n",
    "    'rating': ['Poor', 'Fair', 'Good', 'Excellent', 'Good', 'Fair', 'Excellent']\n",
    "}\n",
    "\n",
    "df_categorical = pd.DataFrame(categorical_data)\n",
    "print(\"Original categorical data:\")\n",
    "print(df_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccd15343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ONE-HOT ENCODING ===\n",
      "One-hot encoded data:\n",
      "      rating  color_Blue  color_Green  color_Red  color_Yellow  size_Large  \\\n",
      "0       Poor       False        False       True         False       False   \n",
      "1       Fair        True        False      False         False       False   \n",
      "2       Good       False         True      False         False        True   \n",
      "3  Excellent       False        False       True         False       False   \n",
      "4       Good        True        False      False         False       False   \n",
      "5       Fair       False        False      False          True        True   \n",
      "6  Excellent       False         True      False         False       False   \n",
      "\n",
      "   size_Medium  size_Small  brand_Adidas  brand_Nike  brand_Puma  \n",
      "0        False        True         False        True       False  \n",
      "1         True       False          True       False       False  \n",
      "2        False       False         False       False        True  \n",
      "3        False        True         False        True       False  \n",
      "4         True       False          True       False       False  \n",
      "5        False       False         False        True       False  \n",
      "6        False        True         False       False        True  \n",
      "\n",
      "Shape: (7, 11)\n",
      "\n",
      "=== LABEL ENCODING (ORDINAL) ===\n",
      "Label encoded ordinal data:\n",
      "      rating  rating_encoded\n",
      "0       Poor               3\n",
      "1       Fair               1\n",
      "2       Good               2\n",
      "3  Excellent               0\n",
      "4       Good               2\n",
      "5       Fair               1\n",
      "6  Excellent               0\n",
      "Label mapping: {'Excellent': np.int64(0), 'Fair': np.int64(1), 'Good': np.int64(2), 'Poor': np.int64(3)}\n"
     ]
    }
   ],
   "source": [
    "# Method 1: One-Hot Encoding\n",
    "print(\"=== ONE-HOT ENCODING ===\")\n",
    "df_encoded = pd.get_dummies(df_categorical, columns=['color', 'size', 'brand'])\n",
    "print(\"One-hot encoded data:\")\n",
    "print(df_encoded)\n",
    "print(f\"\\nShape: {df_encoded.shape}\")\n",
    "\n",
    "# Method 2: Label Encoding (for ordinal data)\n",
    "print(\"\\n=== LABEL ENCODING (ORDINAL) ===\")\n",
    "le = LabelEncoder()\n",
    "df_categorical['rating_encoded'] = le.fit_transform(df_categorical['rating'])\n",
    "print(\"Label encoded ordinal data:\")\n",
    "print(df_categorical[['rating', 'rating_encoded']])\n",
    "print(f\"Label mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4df06340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TARGET ENCODING ===\n",
      "Target encoded data:\n",
      "    brand  target  brand_target_encoded\n",
      "0    Nike       0              0.333333\n",
      "1  Adidas       1              0.500000\n",
      "2    Puma       0              0.000000\n",
      "3    Nike       0              0.333333\n",
      "4  Adidas       0              0.500000\n",
      "5    Nike       1              0.333333\n",
      "6    Puma       0              0.000000\n",
      "\n",
      "Target encoding mapping:\n",
      "{'Adidas': 0.5, 'Nike': 0.3333333333333333, 'Puma': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Target Encoding (with synthetic target)\n",
    "print(\"=== TARGET ENCODING ===\")\n",
    "# Create a synthetic target variable for demonstration\n",
    "np.random.seed(42)\n",
    "df_categorical['target'] = np.random.randint(0, 2, len(df_categorical))\n",
    "\n",
    "# Target encoding for high cardinality categorical variable\n",
    "# Compute mean target for each brand\n",
    "brand_target_mean = df_categorical.groupby('brand')['target'].mean()\n",
    "# Map the mean to each row\n",
    "df_categorical['brand_target_encoded'] = df_categorical['brand'].map(brand_target_mean)\n",
    "\n",
    "print(\"Target encoded data:\")\n",
    "print(df_categorical[['brand', 'target', 'brand_target_encoded']])\n",
    "print(f\"\\nTarget encoding mapping:\")\n",
    "print(brand_target_mean.to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a6c6c",
   "metadata": {},
   "source": [
    "### 6. Видалення дублікатів\n",
    "\n",
    "Дублікати записів можуть спотворити результати аналізу та витратити обчислювальні ресурси. Ідентифікація та обробка дублікатів є критично важливою для якості даних.\n",
    "\n",
    "#### Типи дублікатів\n",
    "\n",
    "**Точні дублікати**\n",
    "- Записи, які ідентичні по всіх стовпцях\n",
    "- **Причина**: Помилки введення даних, збої системи\n",
    "- **Рішення**: Видалити всі, крім одного екземпляра\n",
    "\n",
    "**Часткові дублікати**\n",
    "- Записи, які дуже схожі, але не ідентичні\n",
    "- **Причина**: Невеликі варіації у введенні даних, відмінності у форматуванні\n",
    "- **Рішення**: Використовувати нечітке зіставлення або пороги схожості\n",
    "\n",
    "#### Методи виявлення дублікатів\n",
    "\n",
    "**1. Точне зіставлення**\n",
    "- Порівняння всіх стовпців на точні збіги\n",
    "- **Коли використовувати**: Коли ви очікуєте точні дублікати\n",
    "- **Інструменти**: pandas.duplicated(), SQL DISTINCT\n",
    "\n",
    "**2. Нечітке зіставлення**\n",
    "- Використання алгоритмів схожості для знаходження майже дублікатів\n",
    "- **Коли використовувати**: Коли дані мають невеликі варіації\n",
    "- **Інструменти**: Відстань Левенштейна, схожість Жаккара\n",
    "\n",
    "**3. Зіставлення за бізнес-правилами**\n",
    "- Використання предметних знань для ідентифікації дублікатів\n",
    "- **Коли використовувати**: Коли дублікати мають різні ідентифікатори\n",
    "- **Інструменти**: Користувацька логіка, алгоритми зв'язування записів"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abb19159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with duplicates:\n",
      "   id         name            email  age         city\n",
      "0   1   John Smith   john@email.com   25     New York\n",
      "1   2     Jane Doe   jane@email.com   30  Los Angeles\n",
      "2   3   John Smith   john@email.com   25     New York\n",
      "3   4  Bob Johnson    bob@email.com   35      Chicago\n",
      "4   5     Jane Doe   jane@email.com   30  Los Angeles\n",
      "5   6   John Smith   john@email.com   25     New York\n",
      "6   7  Alice Brown  alice@email.com   28       Boston\n",
      "7   8  Bob Johnson    bob@email.com   35      Chicago\n",
      "\n",
      "Original shape: (8, 5)\n"
     ]
    }
   ],
   "source": [
    "# Duplicate Detection and Removal Examples\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Create dataset with duplicates\n",
    "duplicate_data = {\n",
    "    'id': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    'name': ['John Smith', 'Jane Doe', 'John Smith', 'Bob Johnson', 'Jane Doe', 'John Smith', 'Alice Brown', 'Bob Johnson'],\n",
    "    'email': ['john@email.com', 'jane@email.com', 'john@email.com', 'bob@email.com', 'jane@email.com', 'john@email.com', 'alice@email.com', 'bob@email.com'],\n",
    "    'age': [25, 30, 25, 35, 30, 25, 28, 35],\n",
    "    'city': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York', 'Boston', 'Chicago']\n",
    "}\n",
    "\n",
    "df_duplicates = pd.DataFrame(duplicate_data)\n",
    "print(\"Dataset with duplicates:\")\n",
    "print(df_duplicates)\n",
    "print(f\"\\nOriginal shape: {df_duplicates.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98a77df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXACT DUPLICATE DETECTION ===\n",
      "Exact duplicates found: 0\n",
      "Duplicate rows:\n",
      "Empty DataFrame\n",
      "Columns: [id, name, email, age, city]\n",
      "Index: []\n",
      "\n",
      "After removing exact duplicates: (8, 5)\n",
      "Cleaned data:\n",
      "   id         name            email  age         city\n",
      "0   1   John Smith   john@email.com   25     New York\n",
      "1   2     Jane Doe   jane@email.com   30  Los Angeles\n",
      "2   3   John Smith   john@email.com   25     New York\n",
      "3   4  Bob Johnson    bob@email.com   35      Chicago\n",
      "4   5     Jane Doe   jane@email.com   30  Los Angeles\n",
      "5   6   John Smith   john@email.com   25     New York\n",
      "6   7  Alice Brown  alice@email.com   28       Boston\n",
      "7   8  Bob Johnson    bob@email.com   35      Chicago\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Exact Duplicate Detection\n",
    "print(\"=== EXACT DUPLICATE DETECTION ===\")\n",
    "exact_duplicates = df_duplicates.duplicated()\n",
    "print(f\"Exact duplicates found: {exact_duplicates.sum()}\")\n",
    "print(\"Duplicate rows:\")\n",
    "print(df_duplicates[exact_duplicates])\n",
    "\n",
    "# Remove exact duplicates\n",
    "df_no_exact_duplicates = df_duplicates.drop_duplicates()\n",
    "print(f\"\\nAfter removing exact duplicates: {df_no_exact_duplicates.shape}\")\n",
    "print(\"Cleaned data:\")\n",
    "print(df_no_exact_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d78a473f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUPLICATE DETECTION BY SPECIFIC COLUMNS ===\n",
      "Business duplicates found: 4\n",
      "Business duplicate rows:\n",
      "   id         name           email  age         city\n",
      "2   3   John Smith  john@email.com   25     New York\n",
      "4   5     Jane Doe  jane@email.com   30  Los Angeles\n",
      "5   6   John Smith  john@email.com   25     New York\n",
      "7   8  Bob Johnson   bob@email.com   35      Chicago\n",
      "\n",
      "After removing business duplicates: (4, 5)\n",
      "Cleaned data:\n",
      "   id         name            email  age         city\n",
      "0   1   John Smith   john@email.com   25     New York\n",
      "1   2     Jane Doe   jane@email.com   30  Los Angeles\n",
      "3   4  Bob Johnson    bob@email.com   35      Chicago\n",
      "6   7  Alice Brown  alice@email.com   28       Boston\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Duplicate Detection by Specific Columns\n",
    "print(\"=== DUPLICATE DETECTION BY SPECIFIC COLUMNS ===\")\n",
    "# Check for duplicates based on name and email (business key)\n",
    "business_duplicates = df_duplicates.duplicated(subset=['name', 'email'])\n",
    "print(f\"Business duplicates found: {business_duplicates.sum()}\")\n",
    "print(\"Business duplicate rows:\")\n",
    "print(df_duplicates[business_duplicates])\n",
    "\n",
    "# Remove duplicates based on business key\n",
    "df_no_business_duplicates = df_duplicates.drop_duplicates(subset=['name', 'email'])\n",
    "print(f\"\\nAfter removing business duplicates: {df_no_business_duplicates.shape}\")\n",
    "print(\"Cleaned data:\")\n",
    "print(df_no_business_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bc809a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FUZZY DUPLICATE DETECTION ===\n",
      "Dataset with near duplicates:\n",
      "             name             email\n",
      "0      John Smith    john@email.com\n",
      "1       Jon Smith    john@email.com\n",
      "2        Jane Doe    jane@email.com\n",
      "3         Jane D.    jane@email.com\n",
      "4     Bob Johnson     bob@email.com\n",
      "5  Robert Johnson  robert@email.com\n",
      "\n",
      "Fuzzy duplicates found: 9\n",
      "Rows 0 and 1: name_sim=0.95, email_sim=1.00\n",
      "Rows 0 and 2: name_sim=0.22, email_sim=0.86\n",
      "Rows 0 and 3: name_sim=0.35, email_sim=0.86\n",
      "Rows 0 and 4: name_sim=0.38, email_sim=0.81\n",
      "Rows 1 and 2: name_sim=0.24, email_sim=0.86\n",
      "Rows 1 and 3: name_sim=0.38, email_sim=0.86\n",
      "Rows 1 and 4: name_sim=0.30, email_sim=0.81\n",
      "Rows 2 and 3: name_sim=0.80, email_sim=1.00\n",
      "Rows 4 and 5: name_sim=0.80, email_sim=0.83\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Fuzzy Duplicate Detection\n",
    "print(\"=== FUZZY DUPLICATE DETECTION ===\")\n",
    "\n",
    "# Create data with near duplicates\n",
    "fuzzy_data = {\n",
    "    'name': ['John Smith', 'Jon Smith', 'Jane Doe', 'Jane D.', 'Bob Johnson', 'Robert Johnson'],\n",
    "    'email': ['john@email.com', 'john@email.com', 'jane@email.com', 'jane@email.com', 'bob@email.com', 'robert@email.com']\n",
    "}\n",
    "\n",
    "df_fuzzy = pd.DataFrame(fuzzy_data)\n",
    "print(\"Dataset with near duplicates:\")\n",
    "print(df_fuzzy)\n",
    "\n",
    "# Simple fuzzy matching using string similarity\n",
    "def find_fuzzy_duplicates(df, threshold=0.8):\n",
    "    duplicates = []\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i+1, len(df)):\n",
    "            # Calculate similarity for name\n",
    "            name_sim = SequenceMatcher(None, df.iloc[i]['name'], df.iloc[j]['name']).ratio()\n",
    "            # Calculate similarity for email\n",
    "            email_sim = SequenceMatcher(None, df.iloc[i]['email'], df.iloc[j]['email']).ratio()\n",
    "            \n",
    "            if name_sim > threshold or email_sim > threshold:\n",
    "                duplicates.append((i, j, name_sim, email_sim))\n",
    "    return duplicates\n",
    "\n",
    "fuzzy_duplicates = find_fuzzy_duplicates(df_fuzzy)\n",
    "print(f\"\\nFuzzy duplicates found: {len(fuzzy_duplicates)}\")\n",
    "for dup in fuzzy_duplicates:\n",
    "    print(f\"Rows {dup[0]} and {dup[1]}: name_sim={dup[2]:.2f}, email_sim={dup[3]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71b776",
   "metadata": {},
   "source": [
    "## Очищення даних: Best Practices\n",
    "\n",
    "Очищення даних є ітеративним процесом, який вимагає ретельного розгляду вашого конкретного набору даних та цілей аналізу.\n",
    "\n",
    "### 1. Почніть з дослідження даних\n",
    "- Завжди перевіряйте ваші дані перед очищенням\n",
    "- Розумійте типи даних та розподіли\n",
    "- Ідентифікуйте патерни у відсутніх значеннях та викидах\n",
    "\n",
    "### 2. Виберіть методи на основі контексту\n",
    "- **Відсутні значення**: Розгляньте механізм відсутніх даних (MCAR, MAR, MNAR)\n",
    "- **Викиди**: Визначте, чи є вони помилками або справжніми точками даних\n",
    "- **Масштабування**: Виберіть на основі вимог вашого алгоритму\n",
    "- **Кодування**: Виберіть на основі типу категоріальних даних та кардинальності\n",
    "\n",
    "### 3. Документуйте ваші рішення\n",
    "- Відстежуйте всі застосовані перетворення\n",
    "- Документуйте обґрунтування кожного рішення\n",
    "- Підтримуйте лінію даних для відтворюваності\n",
    "\n",
    "### 4. Валідуйте ваші результати\n",
    "- Перевірте, що перетворення мають сенс\n",
    "- Переконайтеся, що якість даних покращилася\n",
    "- Переконайтеся, що інформація не втрачена без причини\n",
    "\n",
    "### 5. Враховуйте подальший вплив\n",
    "- Подумайте про те, як очищення впливає на ваш аналіз\n",
    "- Переконайтеся, що очищені дані підтримують ваші дослідницькі питання\n",
    "- Збалансуйте якість даних з доступністю даних\n",
    "\n",
    "Пам'ятайте: Мета очищення даних - підготувати ваші дані до аналізу, зберігаючи якомога більше цінної інформації. Немає універсального підходу, тому завжди враховуйте ваш конкретний контекст та вимоги."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-mining-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
